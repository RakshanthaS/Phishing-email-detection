{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sklearn.feature_extraction.text as skft\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as skmetrics\n",
    "import sklearn.pipeline as skpipe\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.naive_bayes as sknb\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and create separate dataframe for phishing and non phishing emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phish_df(my_dir): #dataframe for phishing emails with columns as title.content and label as 1(phish)\n",
    "    titles = []\n",
    "    contents = []\n",
    "    labels = []\n",
    "\n",
    "    for f in os.listdir(os.path.join('phish',my_dir)):\n",
    "            with open(os.path.join('phish', my_dir, f), 'r') as reader:\n",
    "                try:\n",
    "                    c = reader.read()\n",
    "                except:\n",
    "                    continue\n",
    "                contents.append(c)\n",
    "                titles.append(f)\n",
    "                labels.append(1)\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'content': contents, 'label': 1},\n",
    "                        columns = ['label', 'title', 'content'])\n",
    "    return df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_email_list = [r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\20051114\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing0\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing1\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing2\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_lst = []\n",
    "for phish_folder in phish_email_list:\n",
    "    phish_lst.append(create_phish_df(phish_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phish = pd.concat(phish_lst)\n",
    "df_phish = df_phish[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_email_list = [r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron3\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron4\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron5\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron6\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ham_df(my_dir): #create dataframe for non phishing email set(ham means non phish) and label as 0.\n",
    "                                      #The other columns are content and title\n",
    "    titles = []\n",
    "    contents = []\n",
    "    labels = []\n",
    "\n",
    "    for f in os.listdir(os.path.join(my_dir,'ham')):\n",
    "            with open(os.path.join(my_dir, 'ham', f), 'r') as reader:\n",
    "                try:\n",
    "                    c = reader.read()\n",
    "                except:\n",
    "                    continue\n",
    "                contents.append(c)\n",
    "                titles.append(f)\n",
    "                labels.append('0')\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'content': contents, 'label': 0},\n",
    "                        columns = ['label', 'title', 'content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_list = []\n",
    "for ham in ham_email_list:\n",
    "    ham_list.append(create_ham_df(ham))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham = pd.concat(ham_list)\n",
    "df_ham = df_ham[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails = pd.concat([df_ham, df_phish])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails_train, df_emails_test = train_test_split(df_emails, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First text data is cleaned, by tokenising and lemmatising the text using wordnet, removing stop words,removing non-alphabetic strings. Bag-of-words approach (BOW) is used.  We look at the histogram of the words within the text, i.e. considering each word count as a feature.The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                               title  \\\n",
      "0      0  0002.2001-05-25.SA_and_HP.spam.txt   \n",
      "1      0  0004.2001-06-12.SA_and_HP.spam.txt   \n",
      "2      0  0005.2001-06-23.SA_and_HP.spam.txt   \n",
      "3      0  0006.2001-06-25.SA_and_HP.spam.txt   \n",
      "4      0  0008.2001-06-25.SA_and_HP.spam.txt   \n",
      "\n",
      "                                             content  predicted_label  \n",
      "0  Subject: fw : this is the solution i mentioned...                1  \n",
      "1  Subject: spend too much on your phone bill ? 2...                0  \n",
      "2  Subject: discounted mortgage broker 512517\\nmo...                0  \n",
      "3  Subject: looking 4 real fun 211075433222\\ntalk...                0  \n",
      "4  Subject: your membership exchange , issue # 42...                0  \n"
     ]
    }
   ],
   "source": [
    "#frequency distribution\n",
    "text_all = '\\n'.join(df_emails_train.content).lower()\n",
    "stop_words = set(stopwords.words('english')) #remove stopwords like a,an,for,the etc from the text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')#nltk.tokenize.wordpunct_tokenize(text_all)\n",
    "tokens_all = tokenizer.tokenize(text_all)\n",
    "tokens_all = [word for word in tokens_all if word not in stop_words and word != 'font' and word != 'subject']#word not in string.punctuation\n",
    "\n",
    "fd = nltk.probability.FreqDist(tokens_all)\n",
    "\n",
    "phish_text_all = '\\n'.join(df_phish.content).lower()\n",
    "phish_tokens_all = tokenizer.tokenize(phish_text_all)\n",
    "phish_tokens_all = [word for word in phish_tokens_all if word not in stop_words and word != 'font' and word != 'subject']\n",
    "\n",
    "fd_phish = nltk.probability.FreqDist(phish_tokens_all)\n",
    "\n",
    "pipeline = skpipe.Pipeline(\n",
    "    steps = [('vect', skft.CountVectorizer(max_df=0.7)), #convert to numerical feature vectors\n",
    "    ('tfidf', skft.TfidfTransformer()), #term frequency ,inverse document frequency\n",
    "    ('clf', sknb.MultinomialNB())]) #multinomial naive bayes classification\n",
    "\n",
    "df_emails_train, df_emails_test = train_test_split(df_emails, test_size=0.3, random_state=0)\n",
    "pipeline.fit(df_emails_train.content, df_emails_train.label)\n",
    "\n",
    "nb_test_predicted = pipeline.predict(df_emails_test.content)\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "labels = []\n",
    "\n",
    "for f in os.listdir(os.path.join(r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron5\",'spam')):\n",
    "        with open(os.path.join(r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron5\", 'spam', f), 'r') as reader:\n",
    "            try:\n",
    "                c = reader.read()\n",
    "            except:\n",
    "                continue\n",
    "            contents.append(c)\n",
    "            titles.append(f)\n",
    "            labels.append(0)\n",
    "\n",
    "df_spam = pd.DataFrame({'title': titles, 'content': contents, 'label': 0},\n",
    "                    columns = ['label', 'title', 'content'])\n",
    "\n",
    "predictions = pipeline.predict(df_spam.content)\n",
    "\n",
    "df_spam['predicted_label'] = predictions\n",
    "\n",
    "print(df_spam[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675\n",
      "3675\n",
      "[[2700  975]\n",
      " [   0    0]]\n",
      "Accuracy Score : 0.7346938775510204\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.85      3675\n",
      "          1       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.73      0.85      3675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x= list(df_spam.label)\n",
    "y= list(predictions)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "results = confusion_matrix(x,y) \n",
    "#print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(x,y))\n",
    "#print ('Report : ')\n",
    "print(classification_report(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1482    3]\n",
      " [ 114 1395]]\n",
      "Accuracy Score : 0.9609218436873748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.93      1.00      0.96      1485\n",
      "      phish       1.00      0.92      0.96      1509\n",
      "\n",
      "avg / total       0.96      0.96      0.96      2994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions1=list(nb_test_predicted)\n",
    "test_label=list(df_emails_test.label)\n",
    "results = confusion_matrix(test_label, predictions1) \n",
    "#print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(test_label,predictions1))\n",
    "#print ('Report : ')\n",
    "print(classification_report(test_label, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized1_text_model.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

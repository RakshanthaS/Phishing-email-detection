{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sklearn.feature_extraction.text as skft\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as skmetrics\n",
    "import sklearn.pipeline as skpipe\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.naive_bayes as sknb\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and create separate dataframe for phishing and non phishing emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phish_df(my_dir): #dataframe for phishing emails with columns as title.content and label as 1(phish)\n",
    "    titles = []\n",
    "    contents = []\n",
    "    labels = []\n",
    "\n",
    "    for f in os.listdir(os.path.join('phish',my_dir)):\n",
    "            with open(os.path.join('phish', my_dir, f), 'r') as reader:\n",
    "                try:\n",
    "                    c = reader.read()\n",
    "                except:\n",
    "                    continue\n",
    "                contents.append(c)\n",
    "                titles.append(f)\n",
    "                labels.append(1)\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'content': contents, 'label': 1},\n",
    "                        columns = ['label', 'title', 'content'])\n",
    "    return df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_email_list = [r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\20051114\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing0\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing1\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing2\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\phish\\phishing3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_lst = []\n",
    "for phish_folder in phish_email_list:\n",
    "    phish_lst.append(create_phish_df(phish_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phish = pd.concat(phish_lst)\n",
    "df_phish = df_phish[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_email_list = [r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron3\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron4\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron5\", r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron6\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ham_df(my_dir): #create dataframe for non phishing email set(ham means non phish) and label as 0.\n",
    "                                      #The other columns are content and title\n",
    "    titles = []\n",
    "    contents = []\n",
    "    labels = []\n",
    "\n",
    "    for f in os.listdir(os.path.join(my_dir,'ham')):\n",
    "            with open(os.path.join(my_dir, 'ham', f), 'r') as reader:\n",
    "                try:\n",
    "                    c = reader.read()\n",
    "                except:\n",
    "                    continue\n",
    "                contents.append(c)\n",
    "                titles.append(f)\n",
    "                labels.append('0')\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'content': contents, 'label': 0},\n",
    "                        columns = ['label', 'title', 'content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_list = []\n",
    "for ham in ham_email_list:\n",
    "    ham_list.append(create_ham_df(ham))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham = pd.concat(ham_list)\n",
    "df_ham = df_ham[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails = pd.concat([df_ham, df_phish])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails_train, df_emails_test = train_test_split(df_emails, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First text data is cleaned, by tokenising and lemmatising the text using wordnet, removing stop words,removing non-alphabetic strings. Bag-of-words approach (BOW) is used.  We look at the histogram of the words within the text, i.e. considering each word count as a feature.The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                               title  \\\n",
      "0         0  0002.2001-05-25.SA_and_HP.spam.txt   \n",
      "1         0  0004.2001-06-12.SA_and_HP.spam.txt   \n",
      "2         0  0005.2001-06-23.SA_and_HP.spam.txt   \n",
      "3         0  0006.2001-06-25.SA_and_HP.spam.txt   \n",
      "4         0  0008.2001-06-25.SA_and_HP.spam.txt   \n",
      "5         0  0009.2001-06-26.SA_and_HP.spam.txt   \n",
      "6         0  0010.2001-06-28.SA_and_HP.spam.txt   \n",
      "7         0  0011.2001-06-29.SA_and_HP.spam.txt   \n",
      "8         0  0013.2001-06-30.SA_and_HP.spam.txt   \n",
      "9         0  0014.2001-07-04.SA_and_HP.spam.txt   \n",
      "10        0  0015.2001-07-05.SA_and_HP.spam.txt   \n",
      "11        0  0016.2001-07-06.SA_and_HP.spam.txt   \n",
      "12        0  0018.2001-07-13.SA_and_HP.spam.txt   \n",
      "13        0  0020.2001-07-28.SA_and_HP.spam.txt   \n",
      "14        0  0022.2001-08-01.SA_and_HP.spam.txt   \n",
      "15        0  0023.2001-08-01.SA_and_HP.spam.txt   \n",
      "16        0  0024.2001-08-01.SA_and_HP.spam.txt   \n",
      "17        0  0025.2001-08-01.SA_and_HP.spam.txt   \n",
      "18        0  0028.2001-08-02.SA_and_HP.spam.txt   \n",
      "19        0  0029.2001-08-03.SA_and_HP.spam.txt   \n",
      "20        0  0031.2001-08-03.SA_and_HP.spam.txt   \n",
      "21        0  0033.2001-08-05.SA_and_HP.spam.txt   \n",
      "22        0  0035.2001-08-05.SA_and_HP.spam.txt   \n",
      "23        0  0036.2001-08-05.SA_and_HP.spam.txt   \n",
      "24        0  0037.2001-08-05.SA_and_HP.spam.txt   \n",
      "25        0  0038.2001-08-05.SA_and_HP.spam.txt   \n",
      "26        0  0040.2001-08-06.SA_and_HP.spam.txt   \n",
      "27        0  0041.2001-08-18.SA_and_HP.spam.txt   \n",
      "28        0  0042.2001-09-17.SA_and_HP.spam.txt   \n",
      "29        0  0044.2001-09-26.SA_and_HP.spam.txt   \n",
      "...     ...                                 ...   \n",
      "3645      0  5137.2005-07-21.SA_and_HP.spam.txt   \n",
      "3646      0  5138.2005-07-21.SA_and_HP.spam.txt   \n",
      "3647      0  5140.2005-07-21.SA_and_HP.spam.txt   \n",
      "3648      0  5141.2005-07-22.SA_and_HP.spam.txt   \n",
      "3649      0  5142.2005-07-22.SA_and_HP.spam.txt   \n",
      "3650      0  5143.2005-07-22.SA_and_HP.spam.txt   \n",
      "3651      0  5144.2005-07-22.SA_and_HP.spam.txt   \n",
      "3652      0  5145.2005-07-22.SA_and_HP.spam.txt   \n",
      "3653      0  5147.2005-07-22.SA_and_HP.spam.txt   \n",
      "3654      0  5148.2005-07-22.SA_and_HP.spam.txt   \n",
      "3655      0  5149.2005-07-22.SA_and_HP.spam.txt   \n",
      "3656      0  5151.2005-07-22.SA_and_HP.spam.txt   \n",
      "3657      0  5153.2005-07-22.SA_and_HP.spam.txt   \n",
      "3658      0  5154.2005-07-22.SA_and_HP.spam.txt   \n",
      "3659      0  5155.2005-07-22.SA_and_HP.spam.txt   \n",
      "3660      0  5157.2005-07-22.SA_and_HP.spam.txt   \n",
      "3661      0  5158.2005-07-22.SA_and_HP.spam.txt   \n",
      "3662      0  5159.2005-07-22.SA_and_HP.spam.txt   \n",
      "3663      0  5161.2005-07-22.SA_and_HP.spam.txt   \n",
      "3664      0  5162.2005-07-22.SA_and_HP.spam.txt   \n",
      "3665      0  5164.2005-07-22.SA_and_HP.spam.txt   \n",
      "3666      0  5165.2005-07-22.SA_and_HP.spam.txt   \n",
      "3667      0  5166.2005-07-22.SA_and_HP.spam.txt   \n",
      "3668      0  5168.2005-07-22.SA_and_HP.spam.txt   \n",
      "3669      0  5170.2005-07-22.SA_and_HP.spam.txt   \n",
      "3670      0  5171.2005-07-22.SA_and_HP.spam.txt   \n",
      "3671      0  5172.2005-07-22.SA_and_HP.spam.txt   \n",
      "3672      0  5173.2005-07-22.SA_and_HP.spam.txt   \n",
      "3673      0  5174.2005-07-22.SA_and_HP.spam.txt   \n",
      "3674      0  5175.2005-07-22.SA_and_HP.spam.txt   \n",
      "\n",
      "                                                content  predicted_label  \n",
      "0     Subject: fw : this is the solution i mentioned...                1  \n",
      "1     Subject: spend too much on your phone bill ? 2...                0  \n",
      "2     Subject: discounted mortgage broker 512517\\nmo...                0  \n",
      "3     Subject: looking 4 real fun 211075433222\\ntalk...                0  \n",
      "4     Subject: your membership exchange , issue # 42...                0  \n",
      "5     Subject: double your life insurance at no extr...                0  \n",
      "6     Subject: urgent business proposal ,\\nmrs . reg...                0  \n",
      "7     Subject: your membership exchange\\ncontent - t...                0  \n",
      "8     Subject: your membership community charset = i...                0  \n",
      "9     Subject: new accounts # 2 c 6 e\\nthis is a mim...                1  \n",
      "10    Subject: get the best rate on a home loan !\\ni...                0  \n",
      "11    Subject: your membership community charset = i...                0  \n",
      "12    Subject: [ ilug ] we need your assistance to i...                0  \n",
      "13    Subject: your membership community charset = i...                0  \n",
      "14    Subject: a brand new opportunity to work from ...                0  \n",
      "15    Subject: home loans just got better !\\nfree se...                1  \n",
      "16    Subject: major stock play\\namnis systems , inc...                0  \n",
      "17    Subject: major stock play\\namnis systems , inc...                0  \n",
      "18    Subject: your membership exchange , # 441\\nyou...                1  \n",
      "19    Subject: your membership community charset = i...                0  \n",
      "20    Subject: re : information that you have reques...                0  \n",
      "21    Subject: data\\nmessage - id :\\nfrom : jasper 5...                0  \n",
      "22    Subject: legal advice\\n* this message was tran...                0  \n",
      "23    Subject: all your life # 4 c 55\\nthis is a mim...                0  \n",
      "24    Subject: foreign currency trading report\\nvolu...                0  \n",
      "25    Subject: free foreign currency newsletter\\nvol...                0  \n",
      "26    Subject: your membership exchange , # 442\\nyou...                1  \n",
      "27    Subject: add logos and tones to your cell phon...                0  \n",
      "28    Subject: internet connectivity that beats the ...                0  \n",
      "29    Subject: xxx site passwords\\nstop\\npaying for ...                1  \n",
      "...                                                 ...              ...  \n",
      "3645  Subject: hershey â€™ s * vs . ghirardelli * - ta...                1  \n",
      "3646  Subject: coke * or pepsi * ? which cola do you...                0  \n",
      "3647  Subject: re : order\\nreflnance or get a loan a...                0  \n",
      "3648  Subject: re [ 8 ] : talk thread about our tabs...                0  \n",
      "3649  Subject: re [ 7 ] : talk thread about his pill...                0  \n",
      "3650  Subject: re [ 7 ] : question with his pills\\ns...                0  \n",
      "3651  Subject: re [ 3 ] : talk thread about our tabs...                0  \n",
      "3652  Subject: acrobat pro 7 . 0 $ 69 . 95 windows x...                0  \n",
      "3653  Subject: http : / / www . joelpittet . com\\nhe...                1  \n",
      "3654  Subject: a friendly professional online pharma...                1  \n",
      "3655  Subject: http : / / www . jumpsociety . com\\nh...                1  \n",
      "3656  Subject: localized software , all languages av...                0  \n",
      "3657  Subject: exactseek - verify your site submissi...                1  \n",
      "3658  Subject: localized software , all languages av...                0  \n",
      "3659  Subject: localized software , all languages av...                0  \n",
      "3660  Subject: localized software , all languages av...                0  \n",
      "3661  Subject: localized software , all languages av...                0  \n",
      "3662  Subject: localized software , all languages av...                0  \n",
      "3663  Subject: localized software , all languages av...                0  \n",
      "3664  Subject: localized software , all languages av...                0  \n",
      "3665  Subject: tetm : 22 , interest : 3 . 55 %\\nha h...                0  \n",
      "3666  Subject: localized software , all languages av...                0  \n",
      "3667  Subject: instant branded software download\\nso...                0  \n",
      "3668  Subject: localized software , all languages av...                0  \n",
      "3669  Subject: downloadable software\\ndvd x copy pla...                0  \n",
      "3670  Subject: localized software , all languages av...                0  \n",
      "3671  Subject: localized software , all languages av...                0  \n",
      "3672  Subject: branded softs\\nnorton internet securi...                0  \n",
      "3673  Subject: microsoft autoroute 2005 dvd uk - $ 1...                0  \n",
      "3674  Subject: localized software , all languages av...                0  \n",
      "\n",
      "[3675 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#frequency distribution\n",
    "text_all = '\\n'.join(df_emails_train.content).lower()\n",
    "stop_words = set(stopwords.words('english')) #remove stopwords like a,an,for,the etc from the text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')#nltk.tokenize.wordpunct_tokenize(text_all)\n",
    "tokens_all = tokenizer.tokenize(text_all)\n",
    "tokens_all = [word for word in tokens_all if word not in stop_words and word != 'font' and word != 'subject']#word not in string.punctuation\n",
    "\n",
    "fd = nltk.probability.FreqDist(tokens_all)\n",
    "\n",
    "phish_text_all = '\\n'.join(df_phish.content).lower()\n",
    "phish_tokens_all = tokenizer.tokenize(phish_text_all)\n",
    "phish_tokens_all = [word for word in phish_tokens_all if word not in stop_words and word != 'font' and word != 'subject']\n",
    "\n",
    "fd_phish = nltk.probability.FreqDist(phish_tokens_all)\n",
    "\n",
    "pipeline = skpipe.Pipeline(\n",
    "    steps = [('vect', skft.CountVectorizer(max_df=0.7)), #convert to numerical feature vectors\n",
    "    ('tfidf', skft.TfidfTransformer()), #term frequency ,inverse document frequency\n",
    "    ('clf', sknb.MultinomialNB())]) #multinomial naive bayes classification\n",
    "\n",
    "df_emails_train, df_emails_test = train_test_split(df_emails, test_size=0.3, random_state=0)\n",
    "pipeline.fit(df_emails_train.content, df_emails_train.label)\n",
    "\n",
    "nb_test_predicted = pipeline.predict(df_emails_test.content)\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "labels = []\n",
    "\n",
    "for f in os.listdir(os.path.join(r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron5\",'spam')):\n",
    "        with open(os.path.join(r\"C:\\Users\\hp\\Downloads\\Phishing-Detection-master\\Phishing-Detection-master\\enron5\", 'spam', f), 'r') as reader:\n",
    "            try:\n",
    "                c = reader.read()\n",
    "            except:\n",
    "                continue\n",
    "            contents.append(c)\n",
    "            titles.append(f)\n",
    "            labels.append(0)\n",
    "\n",
    "df_spam = pd.DataFrame({'title': titles, 'content': contents, 'label': 0},\n",
    "                    columns = ['label', 'title', 'content'])\n",
    "\n",
    "predictions = pipeline.predict(df_spam.content)\n",
    "\n",
    "df_spam['predicted_label'] = predictions\n",
    "\n",
    "print(df_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675\n",
      "3675\n",
      "[[2700  975]\n",
      " [   0    0]]\n",
      "Accuracy Score : 0.7346938775510204\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.85      3675\n",
      "          1       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.73      0.85      3675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x= list(df_spam.label)\n",
    "y= list(predictions)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "results = confusion_matrix(x,y) \n",
    "#print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(x,y))\n",
    "#print ('Report : ')\n",
    "print(classification_report(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1482    3]\n",
      " [ 114 1395]]\n",
      "Accuracy Score : 0.9609218436873748\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.93      1.00      0.96      1485\n",
      "      phish       1.00      0.92      0.96      1509\n",
      "\n",
      "avg / total       0.96      0.96      0.96      2994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions1=list(nb_test_predicted)\n",
    "test_label=list(df_emails_test.label)\n",
    "results = confusion_matrix(test_label, predictions1) \n",
    "#print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(test_label,predictions1))\n",
    "#print ('Report : ')\n",
    "print(classification_report(test_label, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized1_text_model.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration of Text and URL phishing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An mbox file is inputted.All the html tags are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "html_page=codecs.open(\"phisheg.mbox\",'r','utf-8')\n",
    "soup = BeautifulSoup(html_page,features='lxml')\n",
    "tags = soup.find_all('html')\n",
    "text=str(tags)\n",
    "cleantext = BeautifulSoup(text,\"lxml\").text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First text model is called to the dataframe. text_pred is the result of prediction.text_prob is the prababilty score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0.01673557 0.98326443]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = 'finalized1_text_model.sav'\n",
    "loaded_model_text = pickle.load(open(filename, 'rb'))\n",
    "data=[['1',cleantext]]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data,columns = ['No','content'])\n",
    "\n",
    "predictions = loaded_model_text.predict(df.content)\n",
    "print(predictions)\n",
    "text_pred=predictions[0]\n",
    "pred_prob=loaded_model_text.predict_proba(df.content)\n",
    "print(pred_prob)\n",
    "x=np.shape(pred_prob)\n",
    "pred_prob1=[]\n",
    "for i in range(0,x[0]):\n",
    "    for j in range(0,x[1]):\n",
    "        pred_prob1.append(round(pred_prob[i][j],5))\n",
    "text_prob=pred_prob1[1]        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the ahref tags are found and the urls are appended to the list. Also http/https regular expression are found and the urls are appended to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3D\"http://interact.regions.com.ref14xhed.hgsrr3.net/update/ibsregions/cmserver/verify.cfm\"']\n"
     ]
    }
   ],
   "source": [
    "tags = soup.find_all('a')\n",
    "list1= []\n",
    "for t in tags:\n",
    "    list1.append(t.attrs['href'])\n",
    "\n",
    "href = []\n",
    "[href.append(t.attrs['href']) for t.attrs['href'] in list1 if t.attrs['href'] not in href ] \n",
    "print(href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://interact.regions.com.www14xhed/update/ibsregions/cmserver/verify.cfm', 'http://interact.regions.com.www14xhed/update/ibsregions/cmserver/verify.cfm']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "  \n",
    "def Find(string): \n",
    "    # findall() has been used  \n",
    "    # with valid conditions for urls in string \n",
    "    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string) \n",
    "    return url \n",
    "      \n",
    "# Driver Code \n",
    "url=Find(cleantext)\n",
    "print(url)\n",
    "href.extend(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D\"http://interact.regions.com.ref14xhed.hgsrr3.net/update/ibsregions/cmserver/verify.cfm\"',\n",
       " 'http://interact.regions.com.www14xhed/update/ibsregions/cmserver/verify.cfm',\n",
       " 'http://interact.regions.com.www14xhed/update/ibsregions/cmserver/verify.cfm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = list(dict.fromkeys(href))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D\"http://interact.regions.com.ref14xhed.hgsrr3.net/update/ibsregions/cmserver/verify.cfm\"',\n",
       " 'http://interact.regions.com.www14xhed/update/ibsregions/cmserver/verify.cfm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urls are given for feature extraction. the 5 features are extracted and the dataframe is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=href)\n",
    "df.rename(columns={0:\"URL\"})\n",
    "seperation_of_protocol = df[0].str.split(\"://\",expand = True) #expand argument in the split method will give you a new column\n",
    "seperation_domain_name = seperation_of_protocol[1].str.split(\"/\",1,expand = True)\n",
    "seperation_domain_name.columns=[\"domain_name\",\"address\"]\n",
    "splitted_data = pd.concat([seperation_of_protocol[0],seperation_domain_name],axis=1)\n",
    "splitted_data.columns = ['protocol','domain_name','address']\n",
    "\n",
    "\n",
    "def long_url(l):\n",
    "    l= str(l)\n",
    "    \"\"\"This function is defined in order to differntiate website based on the length of the URL\"\"\"\n",
    "    if len(l) < 54:\n",
    "        return 0\n",
    "    elif len(l) >= 54 and len(l) <= 75:\n",
    "        return 2\n",
    "    return 1\n",
    "\t\n",
    "splitted_data['long_url'] = df[0].apply(long_url) \n",
    "def have_at_symbol(l):\n",
    "    \"\"\"This function is used to check whether the URL contains @ symbol or not\"\"\"\n",
    "    if \"@\" in str(l):\n",
    "        return 1\n",
    "    return 0\n",
    "\t\n",
    "splitted_data['having_@_symbol'] = df[0].apply(have_at_symbol)\n",
    "def redirection(l):\n",
    "    \"\"\"If the url has symbol(//) after protocol then such URL is to be classified as phishing \"\"\"\n",
    "    if \"//\" in str(l):\n",
    "        return 1\n",
    "    return 0\n",
    "\t\n",
    "splitted_data['redirection_//_symbol'] = seperation_of_protocol[1].apply(redirection)\n",
    "def prefix_suffix_seperation(l):\n",
    "    if '-' in str(l):\n",
    "        return 1\n",
    "    return 0\n",
    "splitted_data['prefix_suffix_seperation'] = seperation_domain_name['domain_name'].apply(prefix_suffix_seperation)\n",
    "def sub_domains(l):\n",
    "    l= str(l)\n",
    "    if l.count('.') < 3:\n",
    "        return 0\n",
    "    elif l.count('.') == 3:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "splitted_data['sub_domains'] = splitted_data['domain_name'].apply(sub_domains)\n",
    "features = ['long_url', 'having_@_symbol', 'redirection_//_symbol','prefix_suffix_seperation','sub_domains']\n",
    "X=splitted_data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification is done by loading the model.Since there are many urls many url_pred and url_prob are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "['phishing', 'phishing']\n",
      "[[0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'urlphishing.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "ls = loaded_model.predict(X)\n",
    "print(ls)\n",
    "ref = []\n",
    "for i in range(0,len(ls)):\n",
    "    if ls[i] == 0:\n",
    "        ref.append('non-phishing')\n",
    "    else:\n",
    "        ref.append('phishing')\n",
    "print(ref)\n",
    "url_pred_prob = loaded_model.predict_proba(X)\n",
    "print(url_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finding probability score find the maximum of all scores. For finding prediction find the majority of all url's prediction(if there are 4 urls and 3 are phishing(prediction is 1) then the final prediction is one. If there is no majority then its non-phishing(2 are phishing 2 are non phishing) then result is non phishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text_pred)\n",
    "tup=np.shape(url_pred_prob)\n",
    "n=tup[1]-1\n",
    "x=url_pred_prob[:,[n]]\n",
    "url_prob=float(max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def findMajority(arr, n): \n",
    "    maxCount = 0; \n",
    "    index = -1 # sentinels \n",
    "    for i in range(n): \n",
    "        count = 0\n",
    "        for j in range(n): \n",
    "          \n",
    "            if(arr[i] == arr[j]): \n",
    "                count += 1\n",
    "          \n",
    "        # update maxCount if count of  \n",
    "        # current element is greater \n",
    "        if(count > maxCount): \n",
    "          \n",
    "            maxCount = count \n",
    "            index = i \n",
    "      \n",
    "    # if maxCount is greater than n/2  \n",
    "    # return the corresponding element  \n",
    "    if (maxCount > n//2): \n",
    "        in1=arr[index]\n",
    "      \n",
    "    else: \n",
    "        print(\"No Majority Element\")\n",
    "        in1=0\n",
    "    return in1\n",
    "\n",
    "url_pred=findMajority(ls,len(ls))\n",
    "print(url_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally this algorithm is used for finding the overall output. Combining the outputs of url and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phishing\n"
     ]
    }
   ],
   "source": [
    "def is_phish(text_pred,url_pred,text_prob,url_prob):\n",
    "    if(text_pred==1) and (url_pred==1):\n",
    "        print('phishing')\n",
    "    elif(text_pred==1) and (url_pred==0):\n",
    "        if(text_prob>0.6):\n",
    "            print(\"phishing\")\n",
    "        else:\n",
    "            print(\"non-phishing\")\n",
    "    elif(text_pred==0) and (url_pred==1):\n",
    "        if(url_prob>0.6):\n",
    "            print(\"phishing\")\n",
    "        else:\n",
    "            print(\"Non-phishing\")\n",
    "    else:\n",
    "        print('non_phishing')\n",
    "is_phish(text_pred,url_pred,text_prob,url_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0.98326\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(text_pred)\n",
    "print(url_pred)\n",
    "print(text_prob)\n",
    "print(url_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
